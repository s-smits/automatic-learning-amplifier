[{"question": "How do neural networks (KANs and MLPs) handle the problem of catastrophic forgetting?", "answer": "KANs only remodel regions where data is present on in the current phase, leaving previous regions unchanged, whereas MLPs remodel the whole region after seeing new data samples, leading to catastrophic forgetting."}, {"question": "What are some of the mathematical concepts and equations used in this content?", "answer": "The content covers neural networks (KANs and MLPs), partial differential equations (PDEs), continual learning, catastrophic forgetting, interpretable neural networks, unsupervised learning, and topological invariants (in knot theory). Some of the equations include PDE: uxx + uyy = f(x, y), loss function: loss_pde = \u03b1 * loss_i + loss_b, unsupervised learning: f(x1,..., xd) \u2248 0, and supervised learning: y \u2248 f(x1,..., xd)"}, {"question": "How do KANs and MLPs perform on a 1D regression task composed of 5 Gaussian peaks?", "answer": "KANs only remodel regions where data is present on in the current phase, leaving previous regions unchanged, whereas MLPs remodel the whole region after seeing new data samples, leading to catastrophic forgetting."}, {"question": "What are some of the problem-solving strategies used in this content?", "answer": "The content discusses auto-discovered KAN shapes, pruning KANs to achieve reasonable loss, using KANs to solve PDEs, using KANs to avoid catastrophic forgetting, and using KANs for unsupervised learning."}, {"question": "How do KANs demonstrate their ability to reveal the compositional structures in symbolic formulas?", "answer": "KANs are able to reveal the compositional structures present in these formulas, as well as learn the correct univariate functions, by leveraging the computation graph and learned activation functions."}]