[{"question": "What is the main idea behind the use of B-spline functions to approximate 1D functions in spline theory?", "answer": "The main idea is to use B-spline functions to approximate 1D functions by representing them as a composition of smooth functions."}, {"question": "What is the relationship between test loss and model parameters in neural scaling laws?", "answer": "The relationship is that test loss decreases with more model parameters."}, {"question": "Why do the test losses initially go down and then go up in the U-shape, displaying a bias-variance tradeoff?", "answer": "The test losses initially go down and then go up due to the bias-variance tradeoff, where underfitting (bias) and overfitting (variance) occur."}, {"question": "What is the optimal test loss achieved at, and how does it relate to the number of parameters and data points?", "answer": "The optimal test loss is achieved at the interpolation threshold, where the number of parameters matches the number of data points."}, {"question": "Why do KANs generalize better when they have fewer parameters, and how does this relate to the concept of external versus internal degrees of freedom?", "answer": "KANs generalize better when they have fewer parameters because they have both external and internal degrees of freedom, which allows them to benefit from the fact that they have both. External degrees of freedom (parameters) and internal degrees of freedom (grid points inside an activation function) work together to improve generalization."}]