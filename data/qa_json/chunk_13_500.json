[{"question": "Can KANs achieve lower training/test losses than MLPs, given the same number of parameters?", "answer": "Yes, KANs can achieve lower training/test losses than MLPs, given the same number of parameters. Moreover, we report the (surprisingly compact) shapes of our auto-discovered KANs for special functions in Table 2."}, {"question": "What are the compact representations of KANs for special functions?", "answer": "These compact representations imply the possibility of breaking down a high-dimensional lookup table into several 1D lookup tables, which can potentially save a lot of memory, with the (almost negligible) overhead to perform a few additions at inference."}, {"question": "How do the setups in Section 3.1 and Section 3.2 differ from each other?", "answer": "The setup in Section 3.1 is when we clearly know \u201ctrue\u201d KAN shapes, while the setup in Section 3.2 is when we clearly do not know \u201ctrue\u201d KAN shapes. This part investigates a setup lying in the middle: Given the structure of the dataset, we may construct KANs by hand, but we are not sure if they are optimal."}, {"question": "What is the Pareto frontier in this context?", "answer": "The Pareto frontier is defined as fits that are optimal in the sense of no other fit being both simpler and more accurate."}, {"question": "How do the special functions and their KANs compare in terms of performance?", "answer": "We report the performance of KANs and MLPs on various tasks, including regression and PDE solving, and compare their performance on these tasks."}]